{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification / Round 01\n",
    "<hr size=1>\n",
    "\n",
    "Target: Classify Surface Contour and Echotexture in 1964 US Images\n",
    "\n",
    "Pulled From:\n",
    "(Hello World in Google Drive)\n",
    "1. https://colab.research.google.com/drive/1RaZLIPHXcM0z0MRsdtc-njrb48p9zEts?authuser=1#scrollTo=BdMj8z10Z3Ni\n",
    "\n",
    "Based On:\n",
    "1. https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "2. https://github.com/ImagingInformatics/machine-learning/blob/master/SiiM2019/MLcourse_Notebook_1_update.ipynb\n",
    "\n",
    "Major Modification:\n",
    "\n",
    "Learning Points:\n",
    "1. Tends to have best at around 140 epochs, overfitting at about 160\n",
    "2. Overfitting delayed with higher dropout (0.5, 0.2); but no improvement in accuracy before overfitting\n",
    "3. Cyclical LR (exp 1e-5 to 5 x 1e-3)will learn. Non cyclical with not learn.\n",
    "\n",
    "Model Weights: \n",
    "1. class_best_model_nb1_v1.h5: 0.68 accuracy on test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "#from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import *\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RTX cards\n",
    "# https://medium.com/@noel_kennedy/how-to-use-half-precision-float16-when-training-on-rtx-cards-with-tensorflow-keras-d4033d59f9e4\n",
    "\n",
    "import keras.backend as K\n",
    "dtype='float16'\n",
    "K.set_floatx(dtype)\n",
    "# default is 1e-7 which is too small for float16.  Without adjusting the epsilon, we will get NaN predictions because of divide by zero problems\n",
    "K.set_epsilon(1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple confirmation function\n",
    "def confirm(question):\n",
    "    reply = str(input(question+' (y/n): ')).lower().strip()\n",
    "    if reply[0] == 'y':\n",
    "        return True\n",
    "    if reply[0] == 'n':\n",
    "        return False\n",
    "    else:\n",
    "        return confirm(\"Choices are \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical Learning Rate\n",
    "# https://github.com/bckenstler/CLR/blob/master/clr_callback.py\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=1e-6, max_lr=1e-3, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "clr_triangular = CyclicLR(mode='triangular')\n",
    "clr_exp = CyclicLR(mode='exp_range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimg = cv2.imread(\"output_parenchyma/61604190_parenchyma.png\",0)\\n\\nimg2 = histogram_equalization(img)\\n\\nfigure, ax = plt.subplots(1,2, figsize=(6.4,6.4), dpi=100)\\nax[0].imshow(img, cmap=\\'gray\\')\\nax[1].imshow(img2, cmap=\\'gray\\')\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Histogram Equalization\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_histogram_equalization/py_histogram_equalization.html\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def histogram_equalization(img):\n",
    "    hist,bins = np.histogram(img.flatten(),256,[0,256])\n",
    "    cdf = hist.cumsum()\n",
    "    cdf_m = np.ma.masked_equal(cdf,0)\n",
    "    cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())\n",
    "    cdf = np.ma.filled(cdf_m,0).astype('uint8')\n",
    "    img2 = cdf[img]\n",
    "    return img2\n",
    "\n",
    "\"\"\"\n",
    "img = cv2.imread(\"output_parenchyma/61604190_parenchyma.png\",0)\n",
    "\n",
    "img2 = histogram_equalization(img)\n",
    "\n",
    "figure, ax = plt.subplots(1,2, figsize=(6.4,6.4), dpi=100)\n",
    "ax[0].imshow(img, cmap='gray')\n",
    "ax[1].imshow(img2, cmap='gray')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Things I Tried In The Next Cell:</b>\n",
    "1. Histogram Equalization - does not seem to change training behaviour\n",
    "2. Feeding \"images_raw\", which is the unsegmented image - no change in training behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#populate training and testing sets\n",
    "train_X = []\n",
    "train_Y_surface = []\n",
    "train_Y_parenchyma = []\n",
    "test_X = []\n",
    "test_Y_surface = []\n",
    "test_Y_parenchyma = []\n",
    "\n",
    "with open('csv/US_Report_Master.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0: #ignore header\n",
    "            #print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            # Load Images\n",
    "            # Comment out as necessary, to train either surface or parenchyma\n",
    "            #im = Image.open(\"images_raw/\" + row[0] + \".png\", \"r\")\n",
    "            #im = Image.open(\"output_surface/\" + row[0] + \"_surface.png\", \"r\")\n",
    "            im = Image.open(\"output_parenchyma/\" + row[0] + \"_parenchyma.png\", \"r\")\n",
    "            \n",
    "            im = im.resize((IMG_WIDTH,IMG_HEIGHT))\n",
    "            pixelarray = np.array(list(im.getdata(0)))\n",
    "            \n",
    "            # Comment out to apply histogram equalization\n",
    "            #pixelarray = histogram_equalization(pixelarray)\n",
    "            \n",
    "            pixelarray = np.reshape(pixelarray, (IMG_WIDTH,IMG_HEIGHT))\n",
    "            if row[1] == \"TRAIN\": # TRAIN images\n",
    "                train_X.append(pixelarray)\n",
    "                #load ground truths\n",
    "                train_Y_surface.append(row[2])\n",
    "                train_Y_parenchyma.append(row[3])\n",
    "            else: # TEST images\n",
    "                test_X.append(pixelarray)\n",
    "                #load ground truths\n",
    "                test_Y_surface.append(row[2])\n",
    "                test_Y_parenchyma.append(row[3])\n",
    "\n",
    "# Comment out as necessary, to train either surface or parenchyma\n",
    "train_X = np.array(train_X)\n",
    "#train_Y = np.array(train_Y_surface)\n",
    "train_Y = np.array(train_Y_parenchyma)\n",
    "test_X = np.array(test_X)\n",
    "#test_Y = np.array(test_Y_surface)\n",
    "test_Y = np.array(test_Y_parenchyma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  2\n",
      "Output classes :  ['0' '1']\n",
      "Original label: 0\n",
      "After conversion to one-hot: [1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python\n",
    "\n",
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(train_Y)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_X[200,:,:], cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(train_Y[200]))\n",
    "\n",
    "# Reshape to IMG_HEIGHT x IMG_WIDTH x 1\n",
    "train_X = train_X.reshape(-1, IMG_WIDTH,IMG_HEIGHT, 1)\n",
    "test_X = test_X.reshape(-1, IMG_WIDTH,IMG_HEIGHT, 1)\n",
    "\n",
    "# Normalize 0-255 to 0-1\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255.\n",
    "\n",
    "# Change the labels from categorical to one-hot encoding\n",
    "train_Y_one_hot = to_categorical(train_Y)\n",
    "test_Y_one_hot = to_categorical(test_Y)\n",
    "\n",
    "# Display the change for category label using one-hot encoding\n",
    "print('Original label:', train_Y[0])\n",
    "print('After conversion to one-hot:', train_Y_one_hot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1639, 64, 64, 1)\n",
      "(1639,)\n",
      "(325, 64, 64, 1)\n",
      "(325,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1393, 64, 64, 1) (246, 64, 64, 1) (1393, 2) (246, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split training data to 85/15\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.15, random_state=13)\n",
    "\n",
    "print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Things I Tried In The Next Cell:</b>\n",
    "1. This model from datacamp doesn't seem to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### NEURAL NET\\nnum_classes = nClasses\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(IMG_HEIGHT,IMG_WIDTH,1),padding='same'))\\nmodel.add(LeakyReLU(alpha=0.1))\\nmodel.add(MaxPooling2D((2, 2),padding='same'))\\nmodel.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\\nmodel.add(LeakyReLU(alpha=0.1))\\nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\\nmodel.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\\nmodel.add(LeakyReLU(alpha=0.1))                  \\nmodel.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\\nmodel.add(Flatten())\\nmodel.add(Dense(128, activation='linear'))\\nmodel.add(LeakyReLU(alpha=0.1))                  \\nmodel.add(Dense(num_classes, activation='softmax'))\\n\\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=Adam(lr = 1e-4),metrics=['accuracy'])\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "### NEURAL NET\n",
    "num_classes = nClasses\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(IMG_HEIGHT,IMG_WIDTH,1),padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=Adam(lr = 1e-4),metrics=['accuracy'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Things I Tried In The Next Cell:</b>\n",
    "1. Changing number of conv2d layers between 4 to 8; no change\n",
    "2. Changing number of filters, range from 16-32 until 32-256; no change\n",
    "3. Changing number of dense layers between 1 to 2; no change\n",
    "4. Changing number of neurons in dense layer from 20 to 256; no change\n",
    "5. Changing dropouts from 0.1 to 0.5; no change\n",
    "6. Using Adam at lr 1e-4 to 1e-6; model doesn't converge\n",
    "7. Using SGD at lr 1e-4 to 1e-5; model doesn't converge (Model shows training convergence followed by overfitting when I use cyclical learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ftsvd\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ftsvd\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "### NEURAL NET\n",
    "num_classes = nClasses\n",
    "\n",
    "# Create a basic CNN architecture\n",
    "\n",
    "inputs = Input((IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "\n",
    "# Block1\n",
    "conv1 = Conv2D(16, (3,3), activation = 'relu', padding='same', kernel_initializer='lecun_uniform')(inputs)\n",
    "conv2 = Conv2D(16, (3,3), activation = 'relu', padding='same', kernel_initializer='lecun_uniform')(conv1)\n",
    "max1 = MaxPooling2D((2,2), strides = (2,2))(conv2)\n",
    "\n",
    "# Block2\n",
    "conv3 = Conv2D(32, (3,3), activation = 'relu', padding='same', kernel_initializer='lecun_uniform')(max1)\n",
    "conv4 = Conv2D(32, (3,3), activation = 'relu', padding='same', kernel_initializer='lecun_uniform')(conv3)\n",
    "max2 = MaxPooling2D((2,2), strides = (2,2))(conv4)\n",
    "\n",
    "# Block3\n",
    "conv5 = Conv2D(64, (3,3), activation = 'relu', padding='same', kernel_initializer='lecun_uniform')(max2)\n",
    "conv6 = Conv2D(64, (3,3), activation = 'relu', padding='same', kernel_initializer='lecun_uniform')(conv5)\n",
    "max3 = MaxPooling2D((2,2), strides = (2,2))(conv6)\n",
    "\n",
    "fcn = Flatten()(max3)\n",
    "fcn = Dense(128,activation='relu')(fcn)\n",
    "fcn = Dropout(0.3)(fcn) #0.2\n",
    "fcn = Dense(64,activation='relu')(fcn)\n",
    "fcn = Dropout(0.1)(fcn) #0.1\n",
    "output = Dense(num_classes,activation='softmax')(fcn)\n",
    "\n",
    "model = Model(inputs,output)\n",
    "\n",
    "#model.compile(optimizer=Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), \n",
    "#              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer=SGD(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 604,594\n",
      "Trainable params: 604,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[0.86811441 1.17913669]\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_Y),\n",
    "                                                 train_Y)\n",
    "\n",
    "print(class_weights)\n",
    "#class_weights = [0.5, 10.0]\n",
    "#print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodelh5 = 'class_best_model_nb1_v4.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Things I Tried In The Next Cell:</b>\n",
    "1. Batch size between 1 to 50\n",
    "2. Epochs up 600 (for networks that converge slowly / doesn't converge)\n",
    "3. Augentation with zoom, horizontal flip and vertical flip (not for raw or surface images)\n",
    "4. Cyclical LR is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite previous h5 (y/n): y\n",
      "Epoch 1/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6803 - acc: 0.5704 - val_loss: 0.6749 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67491, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 2/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6802 - acc: 0.5697 - val_loss: 0.6700 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67491 to 0.67005, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 3/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6782 - acc: 0.5726 - val_loss: 0.6739 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.67005\n",
      "Epoch 4/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6809 - acc: 0.5632 - val_loss: 0.6694 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67005 to 0.66937, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 5/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6784 - acc: 0.5711 - val_loss: 0.6701 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.66937\n",
      "Epoch 6/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6795 - acc: 0.5639 - val_loss: 0.6715 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.66937\n",
      "Epoch 7/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6792 - acc: 0.5675 - val_loss: 0.6715 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.66937\n",
      "Epoch 8/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6781 - acc: 0.5675 - val_loss: 0.6720 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.66937\n",
      "Epoch 9/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6795 - acc: 0.5661 - val_loss: 0.6713 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.66937\n",
      "Epoch 10/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6807 - acc: 0.5675 - val_loss: 0.6707 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.66937\n",
      "Epoch 11/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6789 - acc: 0.5711 - val_loss: 0.6735 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.66937\n",
      "Epoch 12/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6813 - acc: 0.5668 - val_loss: 0.6680 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.66937 to 0.66798, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 13/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6770 - acc: 0.5668 - val_loss: 0.6865 - val_acc: 0.5610\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.66798\n",
      "Epoch 14/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6792 - acc: 0.5733 - val_loss: 0.6723 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.66798\n",
      "Epoch 15/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6805 - acc: 0.5647 - val_loss: 0.6735 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.66798\n",
      "Epoch 16/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6763 - acc: 0.5747 - val_loss: 0.6704 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.66798\n",
      "Epoch 17/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6801 - acc: 0.5611 - val_loss: 0.6712 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.66798\n",
      "Epoch 18/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6785 - acc: 0.5690 - val_loss: 0.6719 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.66798\n",
      "Epoch 19/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6788 - acc: 0.5647 - val_loss: 0.6716 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.66798\n",
      "Epoch 20/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6766 - acc: 0.5783 - val_loss: 0.6717 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.66798\n",
      "Epoch 21/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6787 - acc: 0.5733 - val_loss: 0.6713 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.66798\n",
      "Epoch 22/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6797 - acc: 0.5682 - val_loss: 0.6706 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.66798\n",
      "Epoch 23/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6785 - acc: 0.5711 - val_loss: 0.6680 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.66798 to 0.66796, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 24/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6792 - acc: 0.5668 - val_loss: 0.6711 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.66796\n",
      "Epoch 25/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6797 - acc: 0.5654 - val_loss: 0.6712 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.66796\n",
      "Epoch 26/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6799 - acc: 0.5618 - val_loss: 0.6676 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.66796 to 0.66763, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 27/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6810 - acc: 0.5668 - val_loss: 0.6713 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.66763\n",
      "Epoch 28/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6781 - acc: 0.5632 - val_loss: 0.6725 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.66763\n",
      "Epoch 29/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6753 - acc: 0.5783 - val_loss: 0.6708 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.66763\n",
      "Epoch 30/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6784 - acc: 0.5661 - val_loss: 0.6715 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.66763\n",
      "Epoch 31/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6763 - acc: 0.5690 - val_loss: 0.6710 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.66763\n",
      "Epoch 32/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6817 - acc: 0.5603 - val_loss: 0.6713 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.66763\n",
      "Epoch 33/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6771 - acc: 0.5668 - val_loss: 0.6719 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.66763\n",
      "Epoch 34/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6767 - acc: 0.5718 - val_loss: 0.6686 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.66763\n",
      "Epoch 35/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6791 - acc: 0.5568 - val_loss: 0.6654 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.66763 to 0.66542, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 36/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6794 - acc: 0.5668 - val_loss: 0.6742 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.66542\n",
      "Epoch 37/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6793 - acc: 0.5704 - val_loss: 0.6670 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.66542\n",
      "Epoch 38/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6794 - acc: 0.5611 - val_loss: 0.6698 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.66542\n",
      "Epoch 39/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6770 - acc: 0.5747 - val_loss: 0.6695 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.66542\n",
      "Epoch 40/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6808 - acc: 0.5524 - val_loss: 0.6716 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.66542\n",
      "Epoch 41/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6768 - acc: 0.5711 - val_loss: 0.6712 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.66542\n",
      "Epoch 42/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6776 - acc: 0.5726 - val_loss: 0.6716 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.66542\n",
      "Epoch 43/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6769 - acc: 0.5790 - val_loss: 0.6703 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.66542\n",
      "Epoch 44/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6772 - acc: 0.5711 - val_loss: 0.6694 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.66542\n",
      "Epoch 45/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6803 - acc: 0.5611 - val_loss: 0.6690 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.66542\n",
      "Epoch 46/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6798 - acc: 0.5639 - val_loss: 0.6743 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.66542\n",
      "Epoch 47/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6796 - acc: 0.5575 - val_loss: 0.6689 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.66542\n",
      "Epoch 48/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6794 - acc: 0.5632 - val_loss: 0.6716 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.66542\n",
      "Epoch 49/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6721 - acc: 0.5805 - val_loss: 0.6704 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.66542\n",
      "Epoch 50/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6772 - acc: 0.5647 - val_loss: 0.6698 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.66542\n",
      "Epoch 51/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6758 - acc: 0.5654 - val_loss: 0.6686 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.66542\n",
      "Epoch 52/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6761 - acc: 0.5690 - val_loss: 0.6685 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.66542\n",
      "Epoch 53/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6781 - acc: 0.5632 - val_loss: 0.6702 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.66542\n",
      "Epoch 54/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6764 - acc: 0.5704 - val_loss: 0.6706 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.66542\n",
      "Epoch 55/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6763 - acc: 0.5711 - val_loss: 0.6704 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.66542\n",
      "Epoch 56/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6771 - acc: 0.5697 - val_loss: 0.6665 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.66542\n",
      "Epoch 57/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6772 - acc: 0.5726 - val_loss: 0.6736 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.66542\n",
      "Epoch 58/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6782 - acc: 0.5668 - val_loss: 0.6677 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.66542\n",
      "Epoch 59/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6775 - acc: 0.5618 - val_loss: 0.6650 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.66542 to 0.66502, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 60/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6737 - acc: 0.5790 - val_loss: 0.6696 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.66502\n",
      "Epoch 61/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6787 - acc: 0.5618 - val_loss: 0.6730 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.66502\n",
      "Epoch 62/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6772 - acc: 0.5647 - val_loss: 0.6689 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.66502\n",
      "Epoch 63/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6777 - acc: 0.5668 - val_loss: 0.6699 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.66502\n",
      "Epoch 64/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6736 - acc: 0.5833 - val_loss: 0.6687 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.66502\n",
      "Epoch 65/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6791 - acc: 0.5661 - val_loss: 0.6691 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.66502\n",
      "Epoch 66/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6763 - acc: 0.5747 - val_loss: 0.6692 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.66502\n",
      "Epoch 67/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6780 - acc: 0.5596 - val_loss: 0.6695 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.66502\n",
      "Epoch 68/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6788 - acc: 0.5690 - val_loss: 0.6721 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.66502\n",
      "Epoch 69/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6730 - acc: 0.5848 - val_loss: 0.6673 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.66502\n",
      "Epoch 70/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6807 - acc: 0.5568 - val_loss: 0.6693 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.66502\n",
      "Epoch 71/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6770 - acc: 0.5668 - val_loss: 0.6717 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.66502\n",
      "Epoch 72/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6757 - acc: 0.5589 - val_loss: 0.6654 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.66502\n",
      "Epoch 73/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6781 - acc: 0.5654 - val_loss: 0.6716 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.66502\n",
      "Epoch 74/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6745 - acc: 0.5790 - val_loss: 0.6710 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.66502\n",
      "Epoch 75/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6772 - acc: 0.5718 - val_loss: 0.6698 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.66502\n",
      "Epoch 76/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6791 - acc: 0.5654 - val_loss: 0.6695 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.66502\n",
      "Epoch 77/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6731 - acc: 0.5711 - val_loss: 0.6702 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.66502\n",
      "Epoch 78/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6750 - acc: 0.5675 - val_loss: 0.6684 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.66502\n",
      "Epoch 79/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6791 - acc: 0.5589 - val_loss: 0.6696 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.66502\n",
      "Epoch 80/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6715 - acc: 0.5876 - val_loss: 0.6677 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.66502\n",
      "Epoch 81/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6796 - acc: 0.5632 - val_loss: 0.6679 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.66502\n",
      "Epoch 82/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6763 - acc: 0.5733 - val_loss: 0.6699 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.66502\n",
      "Epoch 83/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6787 - acc: 0.5661 - val_loss: 0.6659 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.66502\n",
      "Epoch 84/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6754 - acc: 0.5690 - val_loss: 0.6685 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.66502\n",
      "Epoch 85/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6728 - acc: 0.5769 - val_loss: 0.6689 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.66502\n",
      "Epoch 86/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6795 - acc: 0.5560 - val_loss: 0.6690 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.66502\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6707 - acc: 0.5840 - val_loss: 0.6681 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.66502\n",
      "Epoch 88/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6757 - acc: 0.5639 - val_loss: 0.6688 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.66502\n",
      "Epoch 89/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6765 - acc: 0.5682 - val_loss: 0.6698 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.66502\n",
      "Epoch 90/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6740 - acc: 0.5704 - val_loss: 0.6706 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.66502\n",
      "Epoch 91/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6773 - acc: 0.5618 - val_loss: 0.6711 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.66502\n",
      "Epoch 92/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6738 - acc: 0.5690 - val_loss: 0.6700 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.66502\n",
      "Epoch 93/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6783 - acc: 0.5589 - val_loss: 0.6694 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.66502\n",
      "Epoch 94/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6719 - acc: 0.5747 - val_loss: 0.6676 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.66502\n",
      "Epoch 95/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6778 - acc: 0.5582 - val_loss: 0.6631 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.66502 to 0.66315, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 96/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6782 - acc: 0.5639 - val_loss: 0.6666 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.66315\n",
      "Epoch 97/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6772 - acc: 0.5668 - val_loss: 0.6698 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.66315\n",
      "Epoch 98/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6777 - acc: 0.5596 - val_loss: 0.6701 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.66315\n",
      "Epoch 99/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6720 - acc: 0.5761 - val_loss: 0.6686 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.66315\n",
      "Epoch 100/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6752 - acc: 0.5762 - val_loss: 0.6686 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.66315\n",
      "Epoch 101/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6755 - acc: 0.5661 - val_loss: 0.6683 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.66315\n",
      "Epoch 102/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6741 - acc: 0.5690 - val_loss: 0.6691 - val_acc: 0.6382\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.66315\n",
      "Epoch 103/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6718 - acc: 0.5862 - val_loss: 0.6702 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.66315\n",
      "Epoch 104/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6796 - acc: 0.5496 - val_loss: 0.6643 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.66315\n",
      "Epoch 105/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6768 - acc: 0.5711 - val_loss: 0.6661 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.66315\n",
      "Epoch 106/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6691 - acc: 0.5826 - val_loss: 0.6633 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.66315\n",
      "Epoch 107/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6776 - acc: 0.5632 - val_loss: 0.6638 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.66315\n",
      "Epoch 108/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6745 - acc: 0.5704 - val_loss: 0.6648 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.66315\n",
      "Epoch 109/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6727 - acc: 0.5718 - val_loss: 0.6656 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.66315\n",
      "Epoch 110/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6753 - acc: 0.5776 - val_loss: 0.6670 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.66315\n",
      "Epoch 111/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6735 - acc: 0.5704 - val_loss: 0.6664 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.66315\n",
      "Epoch 112/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6775 - acc: 0.5625 - val_loss: 0.6655 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.66315\n",
      "Epoch 113/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6694 - acc: 0.5769 - val_loss: 0.6684 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.66315\n",
      "Epoch 114/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6744 - acc: 0.5761 - val_loss: 0.6645 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.66315\n",
      "Epoch 115/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6779 - acc: 0.5718 - val_loss: 0.6715 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.66315\n",
      "Epoch 116/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6776 - acc: 0.5704 - val_loss: 0.6733 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.66315\n",
      "Epoch 117/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6726 - acc: 0.5790 - val_loss: 0.6678 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.66315\n",
      "Epoch 118/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6759 - acc: 0.5797 - val_loss: 0.6694 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.66315\n",
      "Epoch 119/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6773 - acc: 0.5424 - val_loss: 0.6651 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.66315\n",
      "Epoch 120/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6714 - acc: 0.5840 - val_loss: 0.6673 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.66315\n",
      "Epoch 121/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6742 - acc: 0.5776 - val_loss: 0.6686 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.66315\n",
      "Epoch 122/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6718 - acc: 0.5769 - val_loss: 0.6674 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.66315\n",
      "Epoch 123/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6695 - acc: 0.5805 - val_loss: 0.6679 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.66315\n",
      "Epoch 124/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6718 - acc: 0.5711 - val_loss: 0.6644 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.66315\n",
      "Epoch 125/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6751 - acc: 0.5639 - val_loss: 0.6734 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.66315\n",
      "Epoch 126/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6768 - acc: 0.5568 - val_loss: 0.6653 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.66315\n",
      "Epoch 127/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6700 - acc: 0.5855 - val_loss: 0.6642 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.66315\n",
      "Epoch 128/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6748 - acc: 0.5697 - val_loss: 0.6682 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.66315\n",
      "Epoch 129/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6732 - acc: 0.5697 - val_loss: 0.6697 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.66315\n",
      "Epoch 130/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6725 - acc: 0.5683 - val_loss: 0.6689 - val_acc: 0.6098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00130: val_loss did not improve from 0.66315\n",
      "Epoch 131/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6749 - acc: 0.5740 - val_loss: 0.6638 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.66315\n",
      "Epoch 132/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6684 - acc: 0.5812 - val_loss: 0.6665 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.66315\n",
      "Epoch 133/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6720 - acc: 0.5747 - val_loss: 0.6659 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.66315\n",
      "Epoch 134/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6776 - acc: 0.5683 - val_loss: 0.6660 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.66315\n",
      "Epoch 135/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6669 - acc: 0.5884 - val_loss: 0.6646 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.66315\n",
      "Epoch 136/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6726 - acc: 0.5683 - val_loss: 0.6647 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.66315\n",
      "Epoch 137/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6782 - acc: 0.5675 - val_loss: 0.6657 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.66315\n",
      "Epoch 138/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6687 - acc: 0.5819 - val_loss: 0.6720 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.66315\n",
      "Epoch 139/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6775 - acc: 0.5625 - val_loss: 0.6713 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.66315\n",
      "Epoch 140/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6716 - acc: 0.5948 - val_loss: 0.6634 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.66315\n",
      "Epoch 141/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6744 - acc: 0.5654 - val_loss: 0.6678 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.66315\n",
      "Epoch 142/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6712 - acc: 0.5711 - val_loss: 0.6657 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.66315\n",
      "Epoch 143/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6757 - acc: 0.5682 - val_loss: 0.6651 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.66315\n",
      "Epoch 144/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6699 - acc: 0.5704 - val_loss: 0.6653 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.66315\n",
      "Epoch 145/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6728 - acc: 0.5776 - val_loss: 0.6654 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.66315\n",
      "Epoch 146/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6681 - acc: 0.5812 - val_loss: 0.6663 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.66315\n",
      "Epoch 147/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6746 - acc: 0.5711 - val_loss: 0.6700 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.66315\n",
      "Epoch 148/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6737 - acc: 0.5840 - val_loss: 0.6626 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00148: val_loss improved from 0.66315 to 0.66262, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 149/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6655 - acc: 0.5862 - val_loss: 0.6634 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.66262\n",
      "Epoch 150/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6762 - acc: 0.5603 - val_loss: 0.6692 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.66262\n",
      "Epoch 151/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6721 - acc: 0.5876 - val_loss: 0.6610 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.66262 to 0.66099, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 152/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6761 - acc: 0.5711 - val_loss: 0.6664 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.66099\n",
      "Epoch 153/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6708 - acc: 0.5869 - val_loss: 0.6674 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.66099\n",
      "Epoch 154/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6739 - acc: 0.5769 - val_loss: 0.6607 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.66099 to 0.66070, saving model to class_best_model_nb1_v4.h5\n",
      "Epoch 155/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6676 - acc: 0.5769 - val_loss: 0.6660 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.66070\n",
      "Epoch 156/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6694 - acc: 0.5898 - val_loss: 0.6653 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.66070\n",
      "Epoch 157/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6760 - acc: 0.5639 - val_loss: 0.6652 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.66070\n",
      "Epoch 158/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6692 - acc: 0.5762 - val_loss: 0.6650 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.66070\n",
      "Epoch 159/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6704 - acc: 0.5783 - val_loss: 0.6655 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.66070\n",
      "Epoch 160/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6719 - acc: 0.5704 - val_loss: 0.6679 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.66070\n",
      "Epoch 161/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6756 - acc: 0.5733 - val_loss: 0.6705 - val_acc: 0.6382\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.66070\n",
      "Epoch 162/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6766 - acc: 0.5762 - val_loss: 0.6634 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.66070\n",
      "Epoch 163/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6699 - acc: 0.5733 - val_loss: 0.6617 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.66070\n",
      "Epoch 164/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6684 - acc: 0.5927 - val_loss: 0.6624 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.66070\n",
      "Epoch 165/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6724 - acc: 0.5661 - val_loss: 0.6627 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.66070\n",
      "Epoch 166/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6683 - acc: 0.5848 - val_loss: 0.6628 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.66070\n",
      "Epoch 167/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6680 - acc: 0.5747 - val_loss: 0.6647 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.66070\n",
      "Epoch 168/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6726 - acc: 0.5697 - val_loss: 0.6666 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.66070\n",
      "Epoch 169/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6655 - acc: 0.6085 - val_loss: 0.6654 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.66070\n",
      "Epoch 170/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6711 - acc: 0.5675 - val_loss: 0.6633 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.66070\n",
      "Epoch 171/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6692 - acc: 0.5927 - val_loss: 0.6629 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.66070\n",
      "Epoch 172/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6714 - acc: 0.5776 - val_loss: 0.6852 - val_acc: 0.5447\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.66070\n",
      "Epoch 173/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6747 - acc: 0.5711 - val_loss: 0.6703 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.66070\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6731 - acc: 0.5754 - val_loss: 0.6613 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.66070\n",
      "Epoch 175/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6742 - acc: 0.5754 - val_loss: 0.6665 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.66070\n",
      "Epoch 176/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6714 - acc: 0.5920 - val_loss: 0.6651 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.66070\n",
      "Epoch 177/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6737 - acc: 0.5668 - val_loss: 0.6638 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.66070\n",
      "Epoch 178/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6703 - acc: 0.5654 - val_loss: 0.6663 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.66070\n",
      "Epoch 179/200\n",
      "348/348 [==============================] - ETA: 0s - loss: 0.6667 - acc: 0.592 - 2s 6ms/step - loss: 0.6669 - acc: 0.5920 - val_loss: 0.6639 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.66070\n",
      "Epoch 180/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6658 - acc: 0.5905 - val_loss: 0.6631 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.66070\n",
      "Epoch 181/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6663 - acc: 0.5912 - val_loss: 0.6647 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.66070\n",
      "Epoch 182/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6697 - acc: 0.5912 - val_loss: 0.6643 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.66070\n",
      "Epoch 183/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6693 - acc: 0.5884 - val_loss: 0.6690 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.66070\n",
      "Epoch 184/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6729 - acc: 0.5761 - val_loss: 0.6639 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.66070\n",
      "Epoch 185/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6654 - acc: 0.5977 - val_loss: 0.6631 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.66070\n",
      "Epoch 186/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6723 - acc: 0.5848 - val_loss: 0.6627 - val_acc: 0.6057\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.66070\n",
      "Epoch 187/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6701 - acc: 0.5912 - val_loss: 0.6643 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.66070\n",
      "Epoch 188/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6711 - acc: 0.5869 - val_loss: 0.6652 - val_acc: 0.6463\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.66070\n",
      "Epoch 189/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6672 - acc: 0.5898 - val_loss: 0.6624 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.66070\n",
      "Epoch 190/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6719 - acc: 0.5740 - val_loss: 0.6640 - val_acc: 0.6179\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.66070\n",
      "Epoch 191/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6671 - acc: 0.5826 - val_loss: 0.6640 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.66070\n",
      "Epoch 192/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6625 - acc: 0.5999 - val_loss: 0.6632 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.66070\n",
      "Epoch 193/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6636 - acc: 0.5912 - val_loss: 0.6648 - val_acc: 0.6220\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.66070\n",
      "Epoch 194/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6710 - acc: 0.5819 - val_loss: 0.6697 - val_acc: 0.6341\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.66070\n",
      "Epoch 195/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6695 - acc: 0.5776 - val_loss: 0.6671 - val_acc: 0.6301\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.66070\n",
      "Epoch 196/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6721 - acc: 0.5797 - val_loss: 0.6636 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.66070\n",
      "Epoch 197/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6660 - acc: 0.5941 - val_loss: 0.6609 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.66070\n",
      "Epoch 198/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6692 - acc: 0.5776 - val_loss: 0.6668 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.66070\n",
      "Epoch 199/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6720 - acc: 0.5855 - val_loss: 0.6648 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.66070\n",
      "Epoch 200/200\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.6658 - acc: 0.5869 - val_loss: 0.6657 - val_acc: 0.6382\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.66070\n"
     ]
    }
   ],
   "source": [
    "BS = 4\n",
    "EPOCHS = 200 #20\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "aug = keras.preprocessing.image.ImageDataGenerator(\n",
    "#    rotation_range=30,\n",
    "#    zoom_range=0.2,\n",
    "#    width_shift_range=0.2,\n",
    "#    height_shift_range=0.2,\n",
    "#    shear_range=0.15,\n",
    "#    vertical_flip=True,\n",
    "#    horizontal_flip=True,\n",
    "#    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=400, verbose=2),\n",
    "    ModelCheckpoint(filepath=bestmodelh5, monitor='val_loss', \n",
    "                    save_best_only=True, verbose=2),\n",
    "    clr_exp\n",
    "]\n",
    "\n",
    "# check for previously trained weights to avoid accidentaly overwrite\n",
    "good_to_go = False\n",
    "\n",
    "if (path.exists(bestmodelh5)) :\n",
    "    if (confirm('Overwrite previous h5')):\n",
    "        good_to_go = True\n",
    "else :\n",
    "    good_to_go = True\n",
    "    \n",
    "if (good_to_go):\n",
    "    results = model.fit_generator(\n",
    "                    aug.flow(train_X, train_label, batch_size=BS),\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=len(train_X) // BS,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1,\n",
    "                    class_weight=class_weights,\n",
    "                    validation_data=(valid_X, valid_label),\n",
    "                    use_multiprocessing=False, \n",
    "                    workers=1\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load previous weights (y/n): n\n"
     ]
    }
   ],
   "source": [
    "# WAIT! THINK ABOUT THIS!!!\n",
    "\n",
    "if (confirm(\"Load previous weights\")):\n",
    "    model.load_weights(bestmodelh5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save current weights (y/n): n\n"
     ]
    }
   ],
   "source": [
    "# WAIT! THINK ABOUT THIS!!!\n",
    "\n",
    "if (confirm(\"Save current weights\")):\n",
    "    model.save(bestmodelh5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(results.history.keys())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(results.history['acc'], 'orange', label='Training accuracy')\n",
    "plt.plot(results.history['val_acc'], 'blue', label='Validation accuracy')\n",
    "plt.plot(results.history['loss'], 'red', label='Training loss')\n",
    "plt.plot(results.history['val_loss'], 'green', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Not good practice to use TEST set now, but I plan to redo the random reclassification of data before final (Still awaiting final reader)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6741120793269231\n",
      "Test accuracy: 0.5907662259615385\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Test set\n",
    "test_eval = model.evaluate(test_X, test_Y_one_hot, verbose=0)\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5317 0.4683]\n",
      " [0.5527 0.4475]\n",
      " [0.5728 0.4275]\n",
      " [0.503  0.497 ]\n",
      " [0.587  0.413 ]\n",
      " [0.6    0.4   ]\n",
      " [0.56   0.4402]\n",
      " [0.5674 0.4326]\n",
      " [0.5903 0.4097]\n",
      " [0.59   0.4102]\n",
      " [0.6025 0.3975]\n",
      " [0.511  0.4888]\n",
      " [0.588  0.4124]\n",
      " [0.5454 0.4548]\n",
      " [0.5396 0.4604]\n",
      " [0.5728 0.4272]\n",
      " [0.543  0.457 ]\n",
      " [0.558  0.4421]\n",
      " [0.545  0.4553]\n",
      " [0.5884 0.4116]\n",
      " [0.4968 0.5034]\n",
      " [0.5366 0.4634]\n",
      " [0.592  0.408 ]\n",
      " [0.5415 0.4587]\n",
      " [0.539  0.461 ]\n",
      " [0.5845 0.4158]\n",
      " [0.524  0.476 ]\n",
      " [0.587  0.4128]\n",
      " [0.57   0.4302]\n",
      " [0.5796 0.4207]\n",
      " [0.54   0.4602]\n",
      " [0.556  0.4436]\n",
      " [0.5693 0.4307]\n",
      " [0.5645 0.4355]\n",
      " [0.612  0.3884]\n",
      " [0.537  0.463 ]\n",
      " [0.524  0.4763]\n",
      " [0.5894 0.4104]\n",
      " [0.5806 0.4194]\n",
      " [0.5713 0.4287]\n",
      " [0.607  0.393 ]\n",
      " [0.5977 0.4026]\n",
      " [0.601  0.399 ]\n",
      " [0.583  0.4167]\n",
      " [0.602  0.3977]\n",
      " [0.5957 0.4043]\n",
      " [0.5225 0.4775]\n",
      " [0.571  0.4292]\n",
      " [0.529  0.4712]\n",
      " [0.5894 0.4104]\n",
      " [0.556  0.4436]\n",
      " [0.594  0.4058]\n",
      " [0.5957 0.4045]\n",
      " [0.579  0.421 ]\n",
      " [0.557  0.443 ]\n",
      " [0.551  0.4492]\n",
      " [0.5264 0.4739]\n",
      " [0.551  0.4495]\n",
      " [0.508  0.492 ]\n",
      " [0.571  0.4292]\n",
      " [0.5625 0.4375]\n",
      " [0.569  0.4312]\n",
      " [0.5776 0.422 ]\n",
      " [0.5796 0.4202]\n",
      " [0.542  0.458 ]\n",
      " [0.598  0.4019]\n",
      " [0.578  0.4219]\n",
      " [0.4905 0.51  ]\n",
      " [0.54   0.4597]\n",
      " [0.592  0.4084]\n",
      " [0.5317 0.4683]\n",
      " [0.5273 0.4724]\n",
      " [0.533  0.4666]\n",
      " [0.512  0.4878]\n",
      " [0.519  0.4807]\n",
      " [0.6035 0.3962]\n",
      " [0.5713 0.429 ]\n",
      " [0.561  0.4387]\n",
      " [0.5557 0.4443]\n",
      " [0.528  0.472 ]\n",
      " [0.589  0.4111]\n",
      " [0.5176 0.4824]\n",
      " [0.565  0.435 ]\n",
      " [0.5693 0.4304]\n",
      " [0.5967 0.403 ]\n",
      " [0.5303 0.4697]\n",
      " [0.582  0.4177]\n",
      " [0.5254 0.4746]\n",
      " [0.56   0.4397]\n",
      " [0.541  0.459 ]\n",
      " [0.5034 0.4966]\n",
      " [0.537  0.4626]\n",
      " [0.5586 0.4414]\n",
      " [0.5815 0.4185]\n",
      " [0.532  0.4678]\n",
      " [0.5693 0.431 ]\n",
      " [0.5425 0.4575]\n",
      " [0.578  0.4219]\n",
      " [0.588  0.4119]\n",
      " [0.5884 0.4114]\n",
      " [0.5864 0.4138]\n",
      " [0.5996 0.4001]\n",
      " [0.53   0.4702]\n",
      " [0.5415 0.4587]\n",
      " [0.563  0.4373]\n",
      " [0.553  0.4468]\n",
      " [0.5176 0.4827]\n",
      " [0.532  0.4678]\n",
      " [0.5894 0.4106]\n",
      " [0.5234 0.4766]\n",
      " [0.582  0.4177]\n",
      " [0.549  0.4514]\n",
      " [0.5425 0.4578]\n",
      " [0.598  0.4016]\n",
      " [0.5625 0.4375]\n",
      " [0.5874 0.4124]\n",
      " [0.582  0.4182]\n",
      " [0.5825 0.4177]\n",
      " [0.5923 0.4077]\n",
      " [0.5547 0.445 ]\n",
      " [0.5273 0.4727]\n",
      " [0.573  0.427 ]\n",
      " [0.571  0.4294]\n",
      " [0.5923 0.4077]\n",
      " [0.58   0.42  ]\n",
      " [0.6    0.3997]\n",
      " [0.586  0.4143]\n",
      " [0.58   0.4202]\n",
      " [0.5947 0.4053]\n",
      " [0.5674 0.4326]\n",
      " [0.5757 0.4246]\n",
      " [0.5713 0.4287]\n",
      " [0.599  0.4006]\n",
      " [0.561  0.4387]\n",
      " [0.5137 0.486 ]\n",
      " [0.603  0.397 ]\n",
      " [0.5605 0.4395]\n",
      " [0.5874 0.4124]\n",
      " [0.5527 0.4473]\n",
      " [0.5874 0.4124]\n",
      " [0.5366 0.4636]\n",
      " [0.56   0.44  ]\n",
      " [0.598  0.402 ]\n",
      " [0.5317 0.4683]\n",
      " [0.603  0.3967]\n",
      " [0.531  0.4692]\n",
      " [0.569  0.431 ]\n",
      " [0.5654 0.4348]\n",
      " [0.573  0.4268]\n",
      " [0.566  0.4343]\n",
      " [0.5947 0.4053]\n",
      " [0.5283 0.4714]\n",
      " [0.5947 0.405 ]\n",
      " [0.585  0.4153]\n",
      " [0.5156 0.4844]\n",
      " [0.5674 0.4329]\n",
      " [0.6    0.4   ]\n",
      " [0.574  0.4255]\n",
      " [0.5537 0.4463]\n",
      " [0.593  0.4072]\n",
      " [0.5205 0.4792]\n",
      " [0.604  0.3962]\n",
      " [0.589  0.4111]\n",
      " [0.553  0.4468]\n",
      " [0.5957 0.4045]\n",
      " [0.515  0.485 ]\n",
      " [0.533  0.4668]\n",
      " [0.5264 0.4739]\n",
      " [0.5913 0.409 ]\n",
      " [0.5776 0.4224]\n",
      " [0.531  0.4692]\n",
      " [0.5947 0.4055]\n",
      " [0.5977 0.4026]\n",
      " [0.558  0.4417]\n",
      " [0.5625 0.4377]\n",
      " [0.531  0.4692]\n",
      " [0.5366 0.4631]\n",
      " [0.5854 0.4143]\n",
      " [0.53   0.4702]\n",
      " [0.5283 0.472 ]\n",
      " [0.5503 0.4495]\n",
      " [0.5273 0.4727]\n",
      " [0.5938 0.4062]\n",
      " [0.522  0.4778]\n",
      " [0.5366 0.4636]\n",
      " [0.57   0.4302]\n",
      " [0.6045 0.3953]\n",
      " [0.574  0.4255]\n",
      " [0.571  0.429 ]\n",
      " [0.5415 0.4583]\n",
      " [0.572  0.4282]\n",
      " [0.537  0.4631]\n",
      " [0.4983 0.5015]\n",
      " [0.6025 0.3977]\n",
      " [0.5293 0.4707]\n",
      " [0.541  0.4592]\n",
      " [0.5728 0.4272]\n",
      " [0.5454 0.4548]\n",
      " [0.5757 0.4243]\n",
      " [0.5596 0.4402]\n",
      " [0.5664 0.4336]\n",
      " [0.5596 0.4407]\n",
      " [0.563  0.4373]\n",
      " [0.5703 0.4297]\n",
      " [0.537  0.4631]\n",
      " [0.5938 0.4065]\n",
      " [0.5083 0.4917]\n",
      " [0.5303 0.4697]\n",
      " [0.607  0.393 ]\n",
      " [0.506  0.4941]\n",
      " [0.475  0.525 ]\n",
      " [0.594  0.4058]\n",
      " [0.6016 0.3984]\n",
      " [0.533  0.4668]\n",
      " [0.5737 0.4263]\n",
      " [0.5527 0.4473]\n",
      " [0.542  0.458 ]\n",
      " [0.5386 0.4614]\n",
      " [0.524  0.476 ]\n",
      " [0.5825 0.4172]\n",
      " [0.598  0.4019]\n",
      " [0.546  0.4539]\n",
      " [0.597  0.4026]\n",
      " [0.559  0.441 ]\n",
      " [0.595  0.405 ]\n",
      " [0.583  0.417 ]\n",
      " [0.509  0.4912]\n",
      " [0.6035 0.3965]\n",
      " [0.5884 0.4114]\n",
      " [0.6    0.4001]\n",
      " [0.5547 0.445 ]\n",
      " [0.526  0.474 ]\n",
      " [0.5874 0.4126]\n",
      " [0.607  0.393 ]\n",
      " [0.6016 0.3984]\n",
      " [0.587  0.4133]\n",
      " [0.5664 0.4336]\n",
      " [0.59   0.4102]\n",
      " [0.57   0.4304]\n",
      " [0.541  0.459 ]\n",
      " [0.5493 0.451 ]\n",
      " [0.5815 0.4187]\n",
      " [0.59   0.4102]\n",
      " [0.5356 0.4646]\n",
      " [0.535  0.4648]\n",
      " [0.5117 0.4883]\n",
      " [0.593  0.4075]\n",
      " [0.5645 0.4355]\n",
      " [0.5625 0.4373]\n",
      " [0.578  0.4219]\n",
      " [0.588  0.412 ]\n",
      " [0.579  0.421 ]\n",
      " [0.531  0.4695]\n",
      " [0.547  0.4531]\n",
      " [0.6    0.3997]\n",
      " [0.5864 0.4136]\n",
      " [0.561  0.439 ]\n",
      " [0.596  0.4038]\n",
      " [0.5493 0.4504]\n",
      " [0.5405 0.4597]\n",
      " [0.5757 0.4243]\n",
      " [0.5845 0.4153]\n",
      " [0.494  0.5063]\n",
      " [0.5845 0.4155]\n",
      " [0.5957 0.404 ]\n",
      " [0.5205 0.4792]\n",
      " [0.519  0.481 ]\n",
      " [0.5435 0.4568]\n",
      " [0.5625 0.4373]\n",
      " [0.5923 0.4077]\n",
      " [0.559  0.441 ]\n",
      " [0.5566 0.4434]\n",
      " [0.5737 0.4265]\n",
      " [0.5386 0.4617]\n",
      " [0.5464 0.4539]\n",
      " [0.56   0.44  ]\n",
      " [0.5723 0.4275]\n",
      " [0.5845 0.4155]\n",
      " [0.5464 0.4536]\n",
      " [0.5327 0.4673]\n",
      " [0.539  0.461 ]\n",
      " [0.5913 0.409 ]\n",
      " [0.5156 0.4844]\n",
      " [0.5137 0.486 ]\n",
      " [0.5757 0.4243]\n",
      " [0.59   0.4102]\n",
      " [0.6025 0.3977]\n",
      " [0.59   0.4102]\n",
      " [0.6035 0.3965]\n",
      " [0.5176 0.4827]\n",
      " [0.572  0.428 ]\n",
      " [0.555  0.4448]\n",
      " [0.584  0.416 ]\n",
      " [0.532  0.4678]\n",
      " [0.5845 0.4158]\n",
      " [0.5664 0.4336]\n",
      " [0.608  0.3923]\n",
      " [0.59   0.41  ]\n",
      " [0.5947 0.405 ]\n",
      " [0.5986 0.4011]\n",
      " [0.556  0.4438]\n",
      " [0.583  0.417 ]\n",
      " [0.582  0.418 ]\n",
      " [0.5493 0.4507]\n",
      " [0.5913 0.409 ]\n",
      " [0.5986 0.4016]\n",
      " [0.581  0.419 ]\n",
      " [0.597  0.4026]\n",
      " [0.5356 0.464 ]\n",
      " [0.5605 0.4392]\n",
      " [0.549  0.4512]\n",
      " [0.5894 0.4104]\n",
      " [0.5493 0.4507]\n",
      " [0.571  0.4294]\n",
      " [0.565  0.435 ]\n",
      " [0.5405 0.4595]\n",
      " [0.5894 0.4106]\n",
      " [0.587  0.4133]\n",
      " [0.59   0.4104]\n",
      " [0.582  0.418 ]\n",
      " [0.511  0.4888]\n",
      " [0.5938 0.4062]\n",
      " [0.596  0.4036]\n",
      " [0.5854 0.4146]\n",
      " [0.5825 0.4175]]\n"
     ]
    }
   ],
   "source": [
    "pred_Y = model.predict(test_X)\n",
    "print(pred_Y)\n",
    "\n",
    "#for i in test_X[:1]:\n",
    "#    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
